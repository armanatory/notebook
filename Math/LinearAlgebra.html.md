<h1>Matrix Course Cheat Sheet</h1>

<h2>Day 1: Core and Intermediate Concepts</h2>

<h3>Vector Operations</h3>
<ul>
  <li><strong>Vector Addition</strong>:  
    <p>&#x1D466; + &#x1D467; = (u<sub>1</sub> + v<sub>1</sub>, u<sub>2</sub> + v<sub>2</sub>, &hellip;, u<sub>n</sub> + v<sub>n</sub>)</p>
    <ul>
      <li><strong>Commutative</strong>: &#x1D466; + &#x1D467; = &#x1D467; + &#x1D466;</li>
      <li><strong>Associative</strong>: (&#x1D466; + &#x1D467;) + &#x1D468; = &#x1D466; + (&#x1D467; + &#x1D468;)</li>
    </ul>
  </li>

  <li><strong>Scalar Multiplication</strong>:  
    <p>c &#x1D466; = (c u<sub>1</sub>, c u<sub>2</sub>, &hellip;, c u<sub>n</sub>)</p>
    <ul>
      <li><strong>Distributive</strong>: c(&#x1D466; + &#x1D467;) = c&#x1D466; + c&#x1D467;</li>
      <li><strong>Associative with Scalars</strong>: c(d&#x1D466;) = (cd)&#x1D466;</li>
    </ul>
  </li>

  <li><strong>Dot Product</strong>:  
    <p>&#x1D466; &bull; &#x1D467; = u<sub>1</sub>v<sub>1</sub> + u<sub>2</sub>v<sub>2</sub> + &hellip; + u<sub>n</sub>v<sub>n</sub></p>
    <ul>
      <li><strong>Commutative</strong>: &#x1D466; &bull; &#x1D467; = &#x1D467; &bull; &#x1D466;</li>
      <li><strong>Distributive</strong>: &#x1D466; &bull; (&#x1D467; + &#x1D468;) = &#x1D466; &bull; &#x1D467; + &#x1D466; &bull; &#x1D468;</li>
    </ul>
  </li>
</ul>

<h3>Length and Angle of Vectors</h3>
<ul>
  <li><strong>Length (Magnitude)</strong>:  
    <p>|&#x1D466;| = &#8730;(&#x1D466; &bull; &#x1D466;) = &#8730;(u<sub>1</sub><sup>2</sup> + u<sub>2</sub><sup>2</sup> + &hellip; + u<sub>n</sub><sup>2</sup>)</p>
  </li>

  <li><strong>Cosine of the Angle Between Vectors</strong>:  
    <p>cos(&#x3B8;) = (&#x1D466; &bull; &#x1D467;) / (|&#x1D466;| |&#x1D467;|)</p>
  </li>

  <li><strong>Cauchy-Schwartz Inequality</strong>:  
    <p>|&#x1D466; &bull; &#x1D467;| &#8804; |&#x1D466;| |&#x1D467;|</p>
  </li>

  <li><strong>Triangle Inequality</strong>:  
    <p>|&#x1D466; + &#x1D467;| &#8804; |&#x1D466;| + |&#x1D467;|</p>
  </li>
</ul>

<h3>Linear Combinations and Span</h3>
<ul>
  <li><strong>Linear Combination</strong>:  
    <p>c<sub>1</sub>&#x1D467;<sub>1</sub> + c<sub>2</sub>&#x1D467;<sub>2</sub> + &hellip; + c<sub>n</sub>&#x1D467;<sub>n</sub></p>
    <p>The span of vectors &#x1D467;<sub>1</sub>, &#x1D467;<sub>2</sub>, &hellip;, &#x1D467;<sub>n</sub> is the set of all possible linear combinations of those vectors.</p>
  </li>
</ul>

<h3>Projections and Orthogonality</h3>
<ul>
  <li><strong>Projection of &#x1D467; onto &#x1D466;</strong>:  
    <p>Proj<sub>&#x1D466;</sub>(&#x1D467;) = (&#x1D466; &bull; &#x1D467; / &#x1D466; &bull; &#x1D466;) &#x1D466;</p>
  </li>

  <li><strong>Orthogonality</strong>:  
    <p>Vectors &#x1D466; and &#x1D467; are orthogonal if: &#x1D466; &bull; &#x1D467; = 0</p>
  </li>

  <li><strong>Pythagorean Theorem (in &#x211D;<sup>n</sup>)</strong>:  
    <p>If &#x1D466; &#x22A5; &#x1D467;, then: |&#x1D466; + &#x1D467;|<sup>2</sup> = |&#x1D466;|<sup>2</sup> + |&#x1D467;|<sup>2</sup></p>
  </li>
</ul>

<h3>Gaussian Elimination</h3>
<ol>
  <li>Write the system of linear equations as an augmented matrix.</li>
  <li>Use <strong>row operations</strong> to convert the matrix into <strong>row echelon form</strong> (REF):
    <ul>
      <li>Swap rows</li>
      <li>Multiply a row by a non-zero scalar</li>
      <li>Add or subtract multiples of one row from another</li>
    </ul>
  </li>
  <li><strong>Back substitution</strong> to find the solution.</li>
</ol>

<h3>Matrix Operations</h3>
<ul>
  <li><strong>Matrix Addition</strong>:  
    <p>
      A + B = 
      <table>
        <tr>
          <td>a<sub>11</sub> + b<sub>11</sub></td>
          <td>a<sub>12</sub> + b<sub>12</sub></td>
        </tr>
        <tr>
          <td>a<sub>21</sub> + b<sub>21</sub></td>
          <td>a<sub>22</sub> + b<sub>22</sub></td>
        </tr>
      </table>
    </p>
  </li>

  <li><strong>Matrix Multiplication</strong>:  
    <p>(AB)<sub>ij</sub> = &#8721;<sub>k</sub> a<sub>ik</sub> b<sub>kj</sub></p>
  </li>

  <li><strong>Transpose</strong>:  
    <p>A<sup>T</sup><sub>ij</sub> = A<sub>ji</sub></p>
  </li>
</ul>

<h3>Key Properties</h3>
<ul>
  <li><strong>Distributive</strong>: A(B + C) = AB + AC</li>
  <li><strong>Associative (Matrix Multiplication)</strong>: A(BC) = (AB)C</li>
  <li><strong>Non-Commutative (Matrix Multiplication)</strong>: AB &#8800; BA</li>
</ul>

<h2>Day 2: Advanced Concepts</h2>

<h3>Matrix Inverses</h3>
<ul>
  <li><strong>Matrix Inverse</strong> (for square matrix A):  
    <p>A<sup>-1</sup>A = I = AA<sup>-1</sup></p>
    <p>A is invertible if and only if det(A) &#8800; 0.</p>
  </li>
</ul>

<h3>Determinants</h3>
<ul>
  <li><strong>Determinant of a 2x2 Matrix</strong>:  
    <p>det(A) = |a  b| = ad - bc</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|c  d|</p>
  </li>

  <li><strong>Properties of Determinants</strong>:
    <ul>
      <li>det(AB) = det(A)det(B)</li>
      <li>det(A<sup>T</sup>) = det(A)</li>
      <li>If det(A) = 0, then A is not invertible.</li>
    </ul>
  </li>
</ul>

<h3>Eigenvalues and Eigenvectors</h3>
<ul>
  <li><strong>Eigenvalue Equation</strong>:  
    <p>A&#x1D467; = &#x3BB;&#x1D467;</p>
    <p>&#x3BB; is an eigenvalue and &#x1D467; is an eigenvector of A.</p>
  </li>

  <li><strong>To find eigenvalues</strong>, solve:  
    <p>det(A - &#x3BB;I) = 0</p>
  </li>

  <li><strong>To find eigenvectors</strong>, solve:  
    <p>(A - &#x3BB;I)&#x1D467; = 0</p>
  </li>
</ul>

<h3>Diagonalization</h3>
<ul>
  <li>A matrix A is diagonalizable if there exists a matrix P such that:  
    <p>A = PDP<sup>-1</sup></p>
    <p>where D is a diagonal matrix of eigenvalues, and the columns of P are the eigenvectors of A.</p>
  </li>
</ul>

<h3>Practice Problems</h3>
<p>Focus on mixed problems that combine concepts like determinants, eigenvalues, and projections.</p>
